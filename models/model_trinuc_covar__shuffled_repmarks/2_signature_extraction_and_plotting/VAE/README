Mischan's autoencoder
---------------------
	input: have to previously (../1_coefficient_scaling/) scale the CI-permuted coefficients (within each iteration and within each DNA repair mark) to [-1, +1] because final decoding layer uses tanh as activation

code
	github.com/lehner-lab/RDGVassociation/tree/main/somatic_component_extraction

	1st run VAE code with random 10% train/validation split, to find optimal hyperparameters
	2nd run VAE with optimal hyperparameters, and without split

	environment Mischan used is saved in the docker image (downloaded in singularity_container/)

	all explained in github's readme; relevant excerpt here:

		### Extractions of Variational Autoencoder derived Components
		VAE_tensorflow1.py:
			Running Variational Autoencoder with dataset split (90 % training and 10% test) to find optimal parameters
		
		VAE_tensorflow1_nosplit_alldata.py:
			Running on complete dataset after finding optimal parameters

	NOTE --> had to remove some code in the end which is used for estimating some KPIs specific for Mischan's case ("golden ICs" stuff) 

output
	mean_encoded layer of the VAE (all positive values because of RelU) is equivalent to the exposures from the NMF
	
	"weights is not so trivial because there is also the bias term in the neural net, so you can't easily compare it. This is why we used Pearson correlations. What could be done (but we haven't) is to train a decision tree or random forest and use SHAP to get some feature importance [Shapley values] for each VAE "signature", but it would be mathematically of course not the same like the weights from the NMF"
