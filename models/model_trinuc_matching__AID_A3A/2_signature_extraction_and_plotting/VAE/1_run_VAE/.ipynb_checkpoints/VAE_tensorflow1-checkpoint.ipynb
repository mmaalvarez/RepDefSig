{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3edccb57-abba-4f6b-b2d9-7a237c40ff1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Modified from Mischan Vali-Pour's Variational Autoencoder\n",
    "---\n",
    "github.com/lehner-lab/RDGVassociation/tree/main/somatic_component_extraction\n",
    "\n",
    "(in turn adapted/modified/inspired from github.com/greenelab/tybalt/blob/master/tybalt_vae.ipynb and \"Hand-On Maschine Learning with Scikit-Learn, Keras & Tensorflow\" by Aurélien Géron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1cc5515-1998-42af-9753-bb5fda4d32e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) \n",
      "[GCC 9.4.0]\n",
      "2.2.4-tf\n",
      "1.15.5\n"
     ]
    }
   ],
   "source": [
    "####import all important stuff\n",
    "\n",
    "## important modules\n",
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "import argparse #for parsing\n",
    "print(sys.version) #print version\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "## keras stuff\n",
    "# Tensorflow 1.15.5 recommended\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Layer, Activation, BatchNormalization\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import metrics, optimizers, losses\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "print(keras.__version__)\n",
    "print(tf.__version__)\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#pandas\n",
    "import pandas as pd\n",
    "\n",
    "# for pearson\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "988a479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "####start getting info\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-l', '--learning_rate',\n",
    "                    default=0.0005,\n",
    "                    help='learning rate of the Adam optimizer')\n",
    "parser.add_argument('-b', '--batch_size',\n",
    "                    default=200,\n",
    "                    help='number of samples to include in each learning batch')\n",
    "parser.add_argument('-e', '--epochs',         \n",
    "                    default=200,\n",
    "                    help='how many times to cycle -- NEW: every epoch a new set of training/validating samples is generated')\n",
    "parser.add_argument('-n', '--num_components',\n",
    "                    default=6,\n",
    "                    help='latent space dimensionality (size)')\n",
    "parser.add_argument('-t', '--dataset_training', \n",
    "                    default='VAE_input/permuted_*.tsv',\n",
    "                    help='training+validation samples (output from generate_..._validating.R)')\n",
    "parser.add_argument('-v', '--validation',\n",
    "                    default=0.1,\n",
    "                    help='random fraction of the dataset_training to keep aside as a validation set; required only for hyperparameter optimization')\n",
    "parser.add_argument('-r', '--dataset_real', \n",
    "                    default='VAE_input/original_and_simposcon_scaled.tsv',\n",
    "                    help='real dataset (i.e. no permutations, just [-1,1]-scaled), put in full name + direc')\n",
    "parser.add_argument('-k', '--kappa',\n",
    "                    default=0.5,\n",
    "                    help='kappa, how strongly to linearly ramp up the KL loss after each epoch')\n",
    "parser.add_argument('-d', '--depth',\n",
    "                    default=1,\n",
    "                    help='define whether there should be a layer between latent and input/ouput layer, if yes depth=2, else depth=1')\n",
    "parser.add_argument('-N', '--nmf',\n",
    "                    default='../../NMF/K',\n",
    "                    help='path+prefix to NMF signatures tables, automatically specified for K=num_components, file names must have a _table.tsv suffix')\n",
    "\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d49e5861-f83e-4676-a8e9-0fa8b80b5dfa",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "######## Set hyper parameters\n",
    "\n",
    "learning_rate = float(args.learning_rate)\n",
    "batch_size = int(args.batch_size)\n",
    "epochs = int(args.epochs)\n",
    "latent_dim = int(args.num_components)\n",
    "dataset_training = args.dataset_training\n",
    "validation_set_percent = float(args.validation)\n",
    "dataset_real = args.dataset_real\n",
    "kappa = float(args.kappa)\n",
    "beta = K.variable(0) #KL loss weighting at first epoch\n",
    "\n",
    "#decide on whether there should be a layer before input and latent layer\n",
    "#make this layer\n",
    "depth = int(args.depth)\n",
    "hidden_dim =  latent_dim*2 #douple size of latent dimensions\n",
    "\n",
    "# Random seed\n",
    "seed = int(np.random.randint(low=0, high=10000, size=1))\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067e98a1-11f9-444e-bc53-f4dbd4e03b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## upload and parse input data\n",
    "\n",
    "## upload [-1,1]-scaled full PERMUTED data (for training)\n",
    "full_df = pd.read_csv(dataset_training, sep='\\t')\n",
    "\n",
    "## upload [-1,1]-scaled ORIGINAL data (for final signature extraction)\n",
    "real_df = pd.read_csv(dataset_real, sep='\\t')\n",
    "# store sample names column, renamed as \"Sample\"\n",
    "sample_id = real_df.drop(real_df.columns[1:], axis=1).rename(columns={'sample_id': 'Sample'})\n",
    "\n",
    "# convert to numpy array\n",
    "train_df_input = np.array(full_df.drop(full_df.columns[0], axis=1))\n",
    "real_df_input = np.array(real_df.drop(real_df.columns[0], axis=1))\n",
    "\n",
    "print(train_df_input.shape)\n",
    "print(real_df_input.shape)\n",
    "\n",
    "# Set architecture dimensions\n",
    "original_dim = train_df_input.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0a2457-c270-4414-a792-8512c96727b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "######## def functions and classes\n",
    "\n",
    "# Function for reparameterization trick to make model differentiable\n",
    "# custom layer to sample the codings given mean and log_var\n",
    "# samples from a normal distribution\n",
    "def sampling(args):\n",
    "    # Function with args required for Keras Lambda function\n",
    "    z_mean, z_log_var = args\n",
    "    # sample epsilon for a normal distribution with same shape as input columns aka phenotypes aka somatic features\n",
    "    epsilon = K.random_normal(shape=tf.shape(z_mean), mean=0.,\n",
    "                              stddev=1)\n",
    "    # estimate latent codings by adding epsilon to mu and standard deviation which was learned\n",
    "    z = z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "    return z\n",
    "\n",
    "## custom layer with loss\n",
    "class CustomVariationalLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "    def vae_loss(self, x_input, x_decoded):\n",
    "        reconstruction_loss = original_dim * \\\n",
    "                              metrics.mse(x_input, x_decoded) #using here mean squared error, multiplying with original dim since tensor already uses mean\n",
    "        kl_loss = -0.5 * K.sum(1 + latent_log_encoded -\n",
    "                               K.square(latent_mean_encoded) -\n",
    "                               K.exp(latent_log_encoded), axis=-1)\n",
    "        return K.mean(reconstruction_loss + (K.get_value(beta) * kl_loss)) #combining reconstruction and KL loss and taking the mean\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        x_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, x_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        return x\n",
    "\n",
    "# implement warmup to slowly ramp up the KL loss (beta=0 is vanilla autoencoder and beta=1 full VAE)\n",
    "# KL will be weighted by KL*beta\n",
    "# modified code from https://github.com/keras-team/keras/issues/2595 from https://github.com/greenelab/tybalt/blob/master/tybalt_vae.ipynb\n",
    "# idea of ladder VAE from https://arxiv.org/abs/1602.02282class WarmUpCallback(Callback):\n",
    "class WarmUpCallback(Callback):\n",
    "    def __init__(self, beta, kappa):\n",
    "        self.beta = beta\n",
    "        self.kappa = kappa \n",
    "    def on_epoch_end(self, epoch, logs={}): #on_epoch_begin, # Behavior on each epoch\n",
    "        if K.get_value(self.beta) <= 1:\n",
    "            K.set_value(self.beta, K.get_value(self.beta) + self.kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805bf1f8-f704-4bb8-8ead-c0b75cf4db0a",
   "metadata": {},
   "source": [
    "VAE\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7311e54-3f85-45be-92d2-f6b291253664",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ENCODER ####\n",
    "#first dense layer to get mean and log var\n",
    "#batch normalization\n",
    "#relu as activation function\n",
    "\n",
    "pheno_input = Input(shape=(original_dim, )) # shape == number of neurons in input layer (==DNA repair mark coefficients)\n",
    "z_shape = latent_dim # number of signatures (==neurons) in latent layer\n",
    "\n",
    "if depth == 1:\n",
    "    latent_mean = Dense(latent_dim,\n",
    "                        kernel_initializer='glorot_uniform')(pheno_input)\n",
    "    latent_log_var = Dense(latent_dim,\n",
    "                           kernel_initializer='glorot_uniform')(pheno_input)\n",
    "elif depth == 2:\n",
    "    hidden_dense = Dense(hidden_dim,\n",
    "                         kernel_initializer='glorot_uniform')(pheno_input)\n",
    "    hidden_dense_batchnorm = BatchNormalization()(hidden_dense)\n",
    "    hidden_enc = Activation('relu')(hidden_dense_batchnorm)\n",
    "    latent_mean = Dense(latent_dim,\n",
    "                         kernel_initializer='glorot_uniform')(hidden_enc)\n",
    "    latent_log_var = Dense(latent_dim,\n",
    "                            kernel_initializer='glorot_uniform')(hidden_enc)\n",
    "\n",
    "# batch normalization and activation for mu and log variance\n",
    "latent_mean_batchnorm = BatchNormalization()(latent_mean)\n",
    "latent_mean_encoded = Activation('relu')(latent_mean_batchnorm)\n",
    "\n",
    "latent_log_batchnorm = BatchNormalization()(latent_log_var)\n",
    "latent_log_encoded = Activation('relu')(latent_log_batchnorm)\n",
    "\n",
    "# reparameterization\n",
    "latent_codings = Lambda(sampling,\n",
    "           output_shape=(z_shape, ))([latent_mean_encoded, latent_log_encoded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9c56cc-c663-455a-82da-72be6a61c959",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DECODER ####\n",
    "#need tanh as activation function since our output values have to be between -1 and 1 (as the input)\n",
    "if depth == 1:\n",
    "    decoder_to_reconstruct = Dense(original_dim,\n",
    "                                   kernel_initializer='glorot_uniform',\n",
    "                                   activation='tanh')\n",
    "elif depth == 2:\n",
    "    decoder_to_reconstruct = Sequential()\n",
    "    decoder_to_reconstruct.add(Dense(hidden_dim,\n",
    "                                     kernel_initializer='glorot_uniform',\n",
    "                                     activation='relu',\n",
    "                                     input_dim=latent_dim))\n",
    "    decoder_to_reconstruct.add(Dense(original_dim,\n",
    "                                     kernel_initializer='glorot_uniform',\n",
    "                                     activation='tanh'))\n",
    "\n",
    "phenos_reconstruct = decoder_to_reconstruct(latent_codings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce444f21-51fc-400a-88b9-bf9eb9552a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### build up autoencoder ###############\n",
    "\n",
    "adam = optimizers.Adam(lr=learning_rate)\n",
    "vae_layer = CustomVariationalLayer()([pheno_input, phenos_reconstruct])\n",
    "vae = Model(pheno_input, vae_layer)\n",
    "vae.compile(optimizer=adam, loss=None, loss_weights=[beta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caae779-8d23-4674-931c-4e55c61ac309",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### fit Model ##########################\n",
    "\n",
    "history = vae.fit(train_df_input,\n",
    "                  shuffle=True,\n",
    "                  epochs=epochs,\n",
    "                  batch_size=batch_size,\n",
    "                  validation_data=(validation_df_input, None),\n",
    "                  callbacks=[WarmUpCallback(beta, kappa)])\n",
    "\n",
    "# evaluate final loss\n",
    "training_loss= vae.evaluate(train_df_input)\n",
    "validation_loss= vae.evaluate(validation_df_input)\n",
    "\n",
    "print(training_loss)\n",
    "print(validation_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1323173e-6199-4856-8797-9293bfbb12b1",
   "metadata": {},
   "source": [
    "QC\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf6a653-ccf3-406f-a688-4e2963eac548",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### check Pearson correlation of reconstruction vs input ###############\n",
    "\n",
    "# encoding \n",
    "encoder = Model(pheno_input, latent_mean_encoded)\n",
    "\n",
    "# extract signatures from validation set\n",
    "val_encoded = encoder.predict_on_batch(validation_df_input)\n",
    "\n",
    "#### decoder generative model\n",
    "decoder_input = Input(shape=(latent_dim, ))  # can generate from any sampled z vector\n",
    "_x_decoded_mean = decoder_to_reconstruct(decoder_input)\n",
    "decoder = Model(decoder_input, _x_decoded_mean)\n",
    "\n",
    "# reconstruction\n",
    "val_reconstructed = decoder.predict(val_encoded) \n",
    "val_reconstructed = pd.DataFrame(val_reconstructed, columns=train_df.drop(train_df.columns[0], axis=1).columns)\n",
    "\n",
    "validation_df= pd.DataFrame(validation_df_input, columns=train_df.drop(train_df.columns[0], axis=1).columns)\n",
    "\n",
    "## check the mean pearson between input and reconstructed, pearson for each sample\n",
    "r = [pearsonr(val_reconstructed.iloc[x, :],\n",
    "              validation_df.iloc[x, :])[0] for x in range(val_reconstructed.shape[0])]\n",
    "r_mean = round(np.mean(np.array(r)), 2)\n",
    "r_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab816100-7fbe-4d93-a816-5f8de034a589",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### get VAE \"signatures\" from real (i.e. not permuted) data #############\n",
    "\n",
    "## Mischan: \"latent_mean_encoded layer of the VAE is equivalent to the exposures from the NMF\"\n",
    "\n",
    "# extract 'signatures' (latent layer means) from original (-1,1 scaled) data\n",
    "encoded_real_df = encoder.predict_on_batch(real_df_input)\n",
    "# to pandas, and rename columns ('signatures')\n",
    "encoded_real_df = pd.DataFrame(encoded_real_df, columns= range(1, latent_dim+1)).add_prefix('vae')\n",
    "# append sample names column\n",
    "encoded_real_df_names = pd.concat([sample_id, encoded_real_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652991b6-3fb9-43b1-9ee6-6129330bc219",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### check Pearson correlation of VAE \"signatures\" vs NMF signatures #############\n",
    "\n",
    "## scale [0,1] every VAE table's row (sample exposures) to compare to NMF scaled exposures\n",
    "encoded_real_df_scaled = encoded_real_df.div(encoded_real_df.sum(axis=1), axis=0)\n",
    "encoded_real_df_scaled_names = pd.concat([sample_id, encoded_real_df_scaled], axis=1)\n",
    "\n",
    "## upload NMF signatures file for K=num_components\n",
    "# pivot wider and other things to have same format as encoded_real_df\n",
    "file_NMF_sig = args.nmf + str(args.num_components) + '_table.tsv'\n",
    "NMF_sig_exp = pd.read_csv(file_NMF_sig, sep='\\t') \\\n",
    "    [['Sample','Signature','signature exposure']] \\\n",
    "    .drop_duplicates() \\\n",
    "    .pivot(index=\"Sample\", columns=\"Signature\", values=\"signature exposure\") \\\n",
    "    .reset_index() \\\n",
    "    .rename_axis(None, axis=1)\n",
    "\n",
    "# store sample names column, as this is later dropped to scale exposures\n",
    "sample_id_nmf = NMF_sig_exp.drop(NMF_sig_exp.columns[1:], axis=1)\n",
    "\n",
    "# scale [0,1] each row (exposures) to compare to VAE scaled latent_mean_encoded\n",
    "# in the process, sample id column has to be dropped\n",
    "NMF_sig_exp_scaled = NMF_sig_exp.select_dtypes(exclude='object') \\\n",
    "    .div(NMF_sig_exp.sum(axis=1, numeric_only = True), axis=0)\n",
    "\n",
    "# re-append sample names column\n",
    "NMF_sig_exp_scaled_names = pd.concat([sample_id, NMF_sig_exp_scaled], axis=1)\n",
    "\n",
    "## combine NMF signature exposures and VAE latent_mean_encoded\n",
    "NMF_VAE_merged = pd.merge(NMF_sig_exp_scaled_names, encoded_real_df_scaled_names, on='Sample')\n",
    "\n",
    "## do pearson correlation between all numeric columns (i.e. either VAE or NMF sig. exposures)\n",
    "pearson_NMF_vs_VAE = NMF_VAE_merged.corr(method='pearson') \\\n",
    "    [['vae1', 'vae2', 'vae3', 'vae4', 'vae5', 'vae6']] \\\n",
    "    .head(latent_dim)\n",
    "\n",
    "# estimate maximum absolute pearson correlation between the VAE mean_latent_encodings and the NMF sigs\n",
    "vae1 = max(abs(pearson_NMF_vs_VAE.vae1.iloc[0:latent_dim]))\n",
    "vae2 = max(abs(pearson_NMF_vs_VAE.vae2.iloc[0:latent_dim]))\n",
    "vae3 = max(abs(pearson_NMF_vs_VAE.vae3.iloc[0:latent_dim]))\n",
    "vae4 = max(abs(pearson_NMF_vs_VAE.vae4.iloc[0:latent_dim]))\n",
    "vae5 = max(abs(pearson_NMF_vs_VAE.vae5.iloc[0:latent_dim]))\n",
    "vae6 = max(abs(pearson_NMF_vs_VAE.vae6.iloc[0:latent_dim]))\n",
    "\n",
    "# create df from it\n",
    "max_pearson_NMF_VAE = pd.DataFrame(np.array([[vae1, vae2, vae3, vae4, vae5, vae6]]),\n",
    "                                   columns=['vae1', 'vae2', 'vae3', 'vae4', 'vae5', 'vae6'])\n",
    "# get mean value (rounded)\n",
    "mean_max_pearson_NMF_VAE = round(max_pearson_NMF_VAE.mean(axis=1), 2).values[0]\n",
    "mean_max_pearson_NMF_VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8721c5f5-8d69-4b11-9bff-8d2fc81e46b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### save outputs ###############\n",
    "\n",
    "# VAE signatures\n",
    "output_df_direc = os.path.join(\"VAE_signatures__\" + str(latent_dim) + \"_components_\" + \n",
    "                               str(learning_rate) + \"_lr_\" + str(batch_size) + \"_batchsize_\" + \n",
    "                               str(epochs) + \"_epochs_\" + str(kappa) + \"_kappa_\" +\n",
    "                               str(1) + \"_beta_\" + str(hidden_dim) + \"_hiddenDim_\" + str(depth) + \"_depth_\" + \n",
    "                               str(r_mean) + \"_Rmean_reconstr_\" + str(mean_max_pearson_NMF_VAE) + \"_Rmean_max_NMF\" + \".tsv\")\n",
    "encoded_real_df_names.to_csv(output_df_direc, sep='\\t', index= False)\n",
    "\n",
    "# pearson correlations (NMF vs VAE signatures) table\n",
    "pearson_NMF_vs_VAE_direc = os.path.join(\"pearson_VAE_NMF__\" + str(latent_dim) + \"_components_\" + \n",
    "                               str(learning_rate) + \"_lr_\" + str(batch_size) + \"_batchsize_\" + \n",
    "                               str(epochs) + \"_epochs_\" + str(kappa) + \"_kappa_\" +\n",
    "                               str(1) + \"_beta_\" + str(hidden_dim) + \"_hiddenDim_\" + str(depth) + \"_depth_\" + \n",
    "                               str(r_mean) + \"_Rmean_reconstr_\" + str(mean_max_pearson_NMF_VAE) + \"_Rmean_max_NMF\" + \".tsv\")\n",
    "pearson_NMF_vs_VAE.to_csv(pearson_NMF_vs_VAE_direc, sep='\\t', index= False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
